MLP Activation Functions â€” Tutorial & Experiments

This repository contains a comprehensive tutorial and experimental analysis on how different activation functions influence the learning performance of a Multilayer Perceptron (MLP). The study compares four activation functions provided by scikit-learn: identity, logistic, tanh, and relu.

Using the scikit-learn Digits dataset, the experiments evaluate each activation function under a fixed architecture and preprocessing pipeline. The objective is to illustrate how activation choices impact model expressiveness, convergence behavior, and classification accuracy.

Project Overview

This repository includes:

MLP Activation Tutorial (PDF) explaining the theory, dataset, preprocessing, experiment setup, and results.

Jupyter Notebook containing all code, visualizations, and evaluation metrics.

Generated plots and CSV outputs to ensure reproducibility.

This project is designed for students, researchers, and educators interested in understanding activation functions and neural-network optimization.
